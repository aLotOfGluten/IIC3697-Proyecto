{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96PapqzwAUkB"
   },
   "source": [
    "# Experimentos de Traducción Portugues -> Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_FHD9kw266t",
    "outputId": "69a8c951-cbb9-443c-b074-99720455b3d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yheiPHx8YN0m"
   },
   "outputs": [],
   "source": [
    "lang = \"por\"\n",
    "n_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5rb6MT0A5mD"
   },
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yFaCwcFLx9HV"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Hg_qHsB95EzQ"
   },
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NF14LcemARSd"
   },
   "outputs": [],
   "source": [
    "!pip install -U accelerate peft transformers sentencepiece datasets\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install nltk\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7-HRnAKmA8LR"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x8Nds_8Y1Mm"
   },
   "source": [
    "## Datasets\n",
    "!REQUIERE SUBIR LOS ARCHIVOS DEL DATASET SINTETICO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smVHNrAeY5ck"
   },
   "source": [
    "Dataset se puede encontrar en el [siguiente enlace](https://github.com/transducens/mayanv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1ViYHauY4v3",
    "outputId": "946c82b0-de5b-4351-debd-9ddb01a3db4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'mayanv' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/transducens/mayanv.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ghoF4-noMjaC"
   },
   "outputs": [],
   "source": [
    "def load_lines(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def generate_dataset(language, train_folder=\"train\", test_folder=\"test\", base_path=\"mayanv/MayanV\"):\n",
    "    # Rutas\n",
    "    train_lang_path = f\"{base_path}/{language}/{train_folder}/data.{language}\"\n",
    "    test_lang_path = f\"{base_path}/{language}/{test_folder}/data.{language}\"\n",
    "\n",
    "    train_es_path = f\"{base_path}/{language}/{train_folder}/data.es\"\n",
    "    test_es_path = f\"{base_path}/{language}/{test_folder}/data.es\"\n",
    "\n",
    "    train_src = load_lines(train_lang_path)\n",
    "    train_tgt = load_lines(train_es_path)\n",
    "\n",
    "    test_src = load_lines(test_lang_path)\n",
    "    test_tgt = load_lines(test_es_path)\n",
    "\n",
    "    # Crea datasets\n",
    "    train_dataset = Dataset.from_dict({\"input\": train_src, \"target\": train_tgt})\n",
    "    test_dataset = Dataset.from_dict({\"input\": test_src, \"target\": test_tgt})\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PmYZbWQ1kPpQ"
   },
   "outputs": [],
   "source": [
    "def invertir_input_target(example):\n",
    "    return {\n",
    "        \"input\": example[\"target\"],\n",
    "        \"target\": example[\"input\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "13f20b199c924755aca8d3a1aabd07a7",
      "aae6e44a75584a3193280d9bb5d1f5bf",
      "4b8fa516036044a9b9ac024bec40a03a",
      "4560211cd7184561bcf0b7593d982e71",
      "e6305597114b49478ec3911665c00f4e",
      "8f0dc1b87f3c45608a4c31b2ed48900d",
      "3d980e6efbbc41258a08ce3c820c3b3f",
      "3a88c4a6376f4bcb94c844a6bad8292a",
      "3c3b995ff7794029a44c2045df2734b4",
      "16bb0c1cc2ee4aa6a5736a7c3bf5b090",
      "d0e91211e23344088501f2e68c3ac025",
      "397c49bb97a84695a104203dc3d6ecdb",
      "0750d75d0bd041a890961e8c1b832240",
      "b3fabd18813b4e648e4136d66e45725e",
      "3c107798cdcb4c599ecc61fb6a852d38",
      "b06a5046dae04d5991a6cd4214231aa4",
      "33aea3c580a84ff98fc7520859b8236a",
      "413a0035d2e845118e36cb308df8992f",
      "fc7526b4e3f84c5db1682336e65934f3",
      "5403f6e13490453aa65f98c40b9ccc80",
      "e89a9ac52aa947a8a62ae73ba4c28af0",
      "387a4d2666a747c3bf00dc1372e5d52c"
     ]
    },
    "id": "4l0xcZcOgsmx",
    "outputId": "71ca96b4-9828-4cf1-a54b-c7811c6990a9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f20b199c924755aca8d3a1aabd07a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397c49bb97a84695a104203dc3d6ecdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "es_qum_dataset = generate_dataset(\"qum\", train_folder=\"test\", test_folder=\"dev\")\n",
    "es_qum_dataset = DatasetDict({\n",
    "    \"train\": es_qum_dataset[\"train\"].map(invertir_input_target),\n",
    "    \"test\": es_qum_dataset[\"test\"].map(invertir_input_target),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "e-p1Rq94bTJL"
   },
   "outputs": [],
   "source": [
    "def generate_en_qum_dataset_from_files(\n",
    "    data_dir=\"./\",\n",
    "    base_path=\"mayanv/MayanV\",\n",
    "    language=\"qum\",\n",
    "    train_folder=\"test\",\n",
    "    test_folder=\"dev\",\n",
    "    lang=\"en\"\n",
    "):\n",
    "    # Paths a los archivos .en\n",
    "    train_en_path = os.path.join(data_dir, f\"train.{lang}\")\n",
    "    test_en_path = os.path.join(data_dir, f\"test.{lang}\")\n",
    "\n",
    "    # Paths a los archivos .qum\n",
    "    train_qum_path = os.path.join(base_path, language, train_folder, \"data.qum\")\n",
    "    test_qum_path = os.path.join(base_path, language, test_folder, \"data.qum\")\n",
    "\n",
    "    # Carga las líneas\n",
    "    train_en = load_lines(train_en_path)\n",
    "    test_en = load_lines(test_en_path)\n",
    "\n",
    "    train_qum = load_lines(train_qum_path)\n",
    "    test_qum = load_lines(test_qum_path)\n",
    "\n",
    "    # Validación\n",
    "    assert len(train_en) == len(train_qum), f\"Train mismatch: {len(train_en)} vs {len(train_qum)}\"\n",
    "    assert len(test_en) == len(test_qum), f\"Test mismatch: {len(test_en)} vs {len(test_qum)}\"\n",
    "\n",
    "    # Combina en datasets Hugging Face\n",
    "    train_dataset = Dataset.from_dict({\"input\": train_en, \"target\": train_qum})\n",
    "    test_dataset = Dataset.from_dict({\"input\": test_en, \"target\": test_qum})\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    })\n",
    "\n",
    "\n",
    "en_qum_dataset = generate_en_qum_dataset_from_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6AMhN2GZpNt"
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4fo2s-t9Zozn"
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    max_length = 128\n",
    "\n",
    "    inputs = finetune_tokenizer(\n",
    "        examples[\"input\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    targets = finetune_tokenizer(\n",
    "        examples[\"target\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def translate_text(model, tokenizer, text, src_lang, tgt_lang):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    forced_bos_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "\n",
    "    # Detecta el dispositivo (GPU o CPU)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Tokeniza y mueve a la misma device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Generación\n",
    "    outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_id)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "def build_and_save_split_en_only(split, model, tokenizer, output_dir):\n",
    "    input_path = os.path.join(output_dir, f\"{split}.{lang}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(input_path, \"w\", encoding=\"utf-8\") as f_en:\n",
    "        print(f\"🔄 Traduciendo y guardando split '{split}' al {lang}...\\n\")\n",
    "\n",
    "        for example in tqdm(es_qum_dataset[split]):\n",
    "            es_text = example[\"input\"].replace(\"#qum#\", \"\").strip()\n",
    "\n",
    "            try:\n",
    "                en_text = translate_text(model, tokenizer, es_text, \"spa_Latn\", f\"{lang}_Latn\")\n",
    "                en_text = \"#qum# \" + en_text.strip()\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error traduciendo: {es_text}\\n{e}\")\n",
    "                en_text = \"#qum#\"\n",
    "\n",
    "            f_en.write(en_text + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Archivo '{split}.{lang}' guardado en {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5ukIdzqYhCo"
   },
   "source": [
    "## Crear dataset sintético\n",
    "!SOLO EJECUTAR SI NO SE TIENE EL DATASET TODAVIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOjTDQjmYq8F"
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "mid_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "mid_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "mid_tokenizer.src_lang = \"spa_Latn\"\n",
    "mid_forced_bos_id = mid_tokenizer.convert_tokens_to_ids(f\"{lang}_Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U4gFjfFYots"
   },
   "outputs": [],
   "source": [
    "output_dir = \"/content/drive/MyDrive/en_qum_dataset\"\n",
    "\n",
    "build_and_save_split_en_only(\"test\", mid_model, mid_tokenizer, output_dir)\n",
    "build_and_save_split_en_only(\"train\", mid_model, mid_tokenizer, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6y8Xh2RBAmS"
   },
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjWCp3bkc27u"
   },
   "source": [
    "### 1. Modelo con Finetuning Español-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0zKtLz_K7Uc"
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "finetune_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "\n",
    "finetune_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Configurar LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Aplicar QLoRA\n",
    "finetune_model = get_peft_model(finetune_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TAeIyJ4pOZM"
   },
   "outputs": [],
   "source": [
    "# Añadir el token especial \"#qum#\" como marcador de idioma\n",
    "special_tokens_dict = {\"additional_special_tokens\": [\"#qum#\"]}\n",
    "finetune_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Redimensionar embeddings del modelo para incluir el nuevo token\n",
    "finetune_model.resize_token_embeddings(len(finetune_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2iPhpoQnSc2"
   },
   "outputs": [],
   "source": [
    "tokenized_train = es_qum_dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "tokenized_test = es_qum_dataset[\"test\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXgDM4lAnagN"
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_finetuned_qum_nllb\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=n_epochs,\n",
    "    learning_rate=2e-4,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_qlora\",\n",
    "    save_total_limit=2,\n",
    "    bf16=False,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finetune_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=finetune_tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "finetune_model.save_pretrained(\"./qlora_finetuned_qum_nllb\")\n",
    "finetune_tokenizer.save_pretrained(\"./qlora_finetuned_qum_nllb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dazLUZlhKKHk"
   },
   "source": [
    "### 2. Modelo con Finetuning Portugues-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEUkIjpj33ln"
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "en_qum_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "en_qum_model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Stma1J90dTmA"
   },
   "outputs": [],
   "source": [
    "# Añadir el token especial \"#qum#\" como marcador de idioma\n",
    "special_tokens_dict = {\"additional_special_tokens\": [\"#qum#\"]}\n",
    "en_qum_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Redimensionar embeddings del modelo para incluir el nuevo token\n",
    "en_qum_model.resize_token_embeddings(len(en_qum_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1sEWghP4B7C"
   },
   "outputs": [],
   "source": [
    "def preprocess_function_en_qum(examples):\n",
    "    max_length = 128\n",
    "    inputs = en_qum_tokenizer(examples[\"input\"], max_length=max_length, truncation=True, padding=True)\n",
    "    targets = en_qum_tokenizer(examples[\"target\"], max_length=max_length, truncation=True, padding=True)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_en_qum_train = en_qum_dataset[\"train\"].map(preprocess_function_en_qum, batched=True)\n",
    "tokenized_en_qum_test = en_qum_dataset[\"test\"].map(preprocess_function_en_qum, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxivqDvt4GEf",
    "outputId": "a9c3425e-49e3-4be1-df1e-0531be05ba4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "training_args_en_qum = TrainingArguments(\n",
    "    output_dir=\"./finetuned_en_qum_nllb\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=n_epochs,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_en_qum\",\n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWYUxQWm4IU_"
   },
   "outputs": [],
   "source": [
    "trainer_en_qum = Trainer(\n",
    "    model=en_qum_model,\n",
    "    args=training_args_en_qum,\n",
    "    train_dataset=tokenized_en_qum_train,\n",
    "    eval_dataset=tokenized_en_qum_test,\n",
    "    tokenizer=en_qum_tokenizer,\n",
    ")\n",
    "\n",
    "trainer_en_qum.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tc2QH1HJcuKn"
   },
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9jCIC05CCG_"
   },
   "source": [
    "### Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEoaaYAZCDnX"
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coKh9lECcw45"
   },
   "source": [
    "### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "m7IRRMdMiMtN"
   },
   "outputs": [],
   "source": [
    "origin_lang = \"por\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjB3SBkqn1JG"
   },
   "source": [
    "#### 1. Con Finetuning Español-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uBCVbR3obZG"
   },
   "outputs": [],
   "source": [
    "predictions_intermediate = []\n",
    "references_intermediate = []\n",
    "\n",
    "print(f\"\\n🔹 Traduciendo: {origin_lang} → Español → Sipakapense\\n\")\n",
    "\n",
    "for item in tqdm(en_qum_dataset[\"test\"]):\n",
    "    input_text = item[\"input\"]\n",
    "    reference = item[\"target\"]\n",
    "\n",
    "    # Paso 1: origin_lang → español\n",
    "    inter = translate_text(finetune_model, finetune_tokenizer, input_text, f\"{origin_lang}_Latn\", \"spa_Latn\")\n",
    "\n",
    "    # Paso 2: español → \"Sipakapense\"\n",
    "    pred = translate_text(finetune_model, finetune_tokenizer, inter, \"spa_Latn\", \"qum\")\n",
    "\n",
    "    predictions_intermediate.append(pred)\n",
    "    references_intermediate.append(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "qVv0v6pUvm-e"
   },
   "outputs": [],
   "source": [
    "clean_pred_intermediate = [pred.removeprefix(\"qum\").strip() for pred in predictions_intermediate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2roPLoJoy2L"
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Ejemplos Fine-Tuned ===\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"> Entrada:     {en_qum_dataset['test'][i]['input']}\")\n",
    "    print(f\"> Referencia:  {en_qum_dataset['test'][i]['target']}\")\n",
    "    print(f\"> Predicción:  {clean_pred_intermediate[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcM-8kilozFz"
   },
   "outputs": [],
   "source": [
    "bleu_score = bleu.compute(predictions=clean_pred_intermediate, references=[[ref] for ref in references_intermediate])\n",
    "\n",
    "print(\"\\n=== Métricas BLEU ===\")\n",
    "print(f\"BLEU-1: {bleu_score['precisions'][0]*100:.2f}\")\n",
    "print(f\"BLEU-2: {bleu_score['precisions'][1]*100:.2f}\")\n",
    "print(f\"BLEU-3: {bleu_score['precisions'][2]*100:.2f}\")\n",
    "print(f\"BLEU-4: {bleu_score['precisions'][3]*100:.2f}\")\n",
    "print(f\"BLEU total: {bleu_score['bleu']*100:.2f}\")\n",
    "\n",
    "rouge_score = rouge.compute(predictions=clean_pred_intermediate, references=references_intermediate)\n",
    "\n",
    "print(\"\\n=== Métricas ROUGE ===\")\n",
    "print(f\"ROUGE-1:   {rouge_score['rouge1']*100:.2f}\")\n",
    "print(f\"ROUGE-2:   {rouge_score['rouge2']*100:.2f}\")\n",
    "print(f\"ROUGE-L:   {rouge_score['rougeL']*100:.2f}\")\n",
    "print(f\"ROUGE-Lsum:{rouge_score['rougeLsum']*100:.2f}\")\n",
    "\n",
    "meteor_score = meteor.compute(predictions=clean_pred_intermediate, references=references_intermediate)\n",
    "\n",
    "print(\"\\n=== Otras Métricas ===\")\n",
    "print(f\"METEOR:     {meteor_score['meteor']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MTsr6mJBWrm"
   },
   "outputs": [],
   "source": [
    "with open(f\"results_{lang}_qum_finetune_intermediate.tsv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerow([\"input\", \"reference\", \"prediction\"])\n",
    "\n",
    "    for example, pred, ref in zip(es_qum_dataset[\"test\"], predictions_intermediate, references_intermediate):\n",
    "        writer.writerow([example[\"input\"], ref, pred])\n",
    "\n",
    "print(\"✅ Resultados guardados en 'results_es_qum_finetune_intermediate.tsv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_UFY-dToFsb"
   },
   "source": [
    "#### 2. Con Finetuning Portugues-Sipakapense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0UrnNGqobuo"
   },
   "outputs": [],
   "source": [
    "predictions_direct = []\n",
    "references_direct = []\n",
    "\n",
    "print(f\"\\n🔹 Traduciendo: {origin_lang} → Sipakapense\\n\")\n",
    "\n",
    "for item in tqdm(en_qum_dataset[\"test\"]):\n",
    "    input_text = item[\"input\"]\n",
    "    reference = item[\"target\"]\n",
    "\n",
    "    pred = translate_text(finetune_model, finetune_tokenizer, input_text, f\"{origin_lang}_Latn\", \"qum\")\n",
    "\n",
    "    predictions_direct.append(pred)\n",
    "    references_direct.append(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "chF42cMCu0k2"
   },
   "outputs": [],
   "source": [
    "clean_pred_direct = [pred.removeprefix(\"qum\").strip() for pred in predictions_direct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGGj_XKctmn9"
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Ejemplos Fine-Tuned ===\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"> Entrada:     {en_qum_dataset['test'][i]['input']}\")\n",
    "    print(f\"> Referencia:  {en_qum_dataset['test'][i]['target']}\")\n",
    "    print(f\"> Predicción:  {clean_pred_direct[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNuiAUOytvUu"
   },
   "outputs": [],
   "source": [
    "bleu_score = bleu.compute(predictions=clean_pred_direct, references=[[ref] for ref in references_direct])\n",
    "\n",
    "print(\"\\n=== Métricas BLEU ===\")\n",
    "print(f\"BLEU-1: {bleu_score['precisions'][0]*100:.2f}\")\n",
    "print(f\"BLEU-2: {bleu_score['precisions'][1]*100:.2f}\")\n",
    "print(f\"BLEU-3: {bleu_score['precisions'][2]*100:.2f}\")\n",
    "print(f\"BLEU-4: {bleu_score['precisions'][3]*100:.2f}\")\n",
    "print(f\"BLEU total: {bleu_score['bleu']*100:.2f}\")\n",
    "\n",
    "rouge_score = rouge.compute(predictions=clean_pred_direct, references=references_direct)\n",
    "\n",
    "print(\"\\n=== Métricas ROUGE ===\")\n",
    "print(f\"ROUGE-1:   {rouge_score['rouge1']*100:.2f}\")\n",
    "print(f\"ROUGE-2:   {rouge_score['rouge2']*100:.2f}\")\n",
    "print(f\"ROUGE-L:   {rouge_score['rougeL']*100:.2f}\")\n",
    "print(f\"ROUGE-Lsum:{rouge_score['rougeLsum']*100:.2f}\")\n",
    "\n",
    "meteor_score = meteor.compute(predictions=clean_pred_direct, references=references_direct)\n",
    "\n",
    "print(\"\\n=== Otras Métricas ===\")\n",
    "print(f\"METEOR:     {meteor_score['meteor']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2o0_ZKITBrRf"
   },
   "outputs": [],
   "source": [
    "filename = f\"results_{lang}_qum_finetune_direct.tsv\"\n",
    "with open(filename, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerow([\"input\", \"reference\", \"prediction\"])\n",
    "\n",
    "    for example, pred, ref in zip(es_qum_dataset[\"test\"], predictions_direct, references_direct):\n",
    "        writer.writerow([example[\"input\"], ref, pred])\n",
    "\n",
    "print(f\"✅ Resultados guardados en '{filename}'\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "g5rb6MT0A5mD",
    "R6AMhN2GZpNt",
    "Q5ukIdzqYhCo",
    "o6ngW-l5c1aq",
    "qjWCp3bkc27u",
    "dazLUZlhKKHk",
    "tc2QH1HJcuKn",
    "U9jCIC05CCG_",
    "coKh9lECcw45",
    "ljNvMeB3ezyA",
    "rjB3SBkqn1JG",
    "y_UFY-dToFsb",
    "fCaQTajaoPyr"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
